{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "class GoDataset(Dataset):\n",
    "    def __init__(self, path_of_data, length):\n",
    "        \"\"\"\n",
    "        Initializes the GoDataset with the given CSV file path.\n",
    "        Args:\n",
    "            path (str): Path to the CSV file containing Go game data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.path = path_of_data\n",
    "        self.preprocessed_path = \"data/preprocessed data\"\n",
    "        self.length = length\n",
    "        self.char2idx = {c: i for i, c in enumerate(\"abcdefghijklmnopqrs\")}\n",
    "        self.dir_len = len(os.listdir('data/preprocessed data'))\n",
    "\n",
    "        # Load data from CSV file\n",
    "        with open(self.path, newline=\"\") as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=\",\")\n",
    "            # Read row by row\n",
    "            self.data = list(reader)  # dtype: list[str]\n",
    "\n",
    "    def __read_from_file(self, row):\n",
    "        random_start = np.random.randint(2, len(row) - self.length)\n",
    "        boards = []\n",
    "        for step in range(random_start, random_start + self.length):\n",
    "            board = torch.zeros((19, 19, 2), dtype=torch.float32)\n",
    "            dim = 0 if row[step][0] == \"B\" else 1\n",
    "            x = self.char2idx[row[step][2]]\n",
    "            y = self.char2idx[row[step][3]]\n",
    "            board[x, y, dim] = 1\n",
    "            boards.append(board)\n",
    "        boards = torch.stack(boards)\n",
    "\n",
    "        dim = 0 if row[random_start + self.length][0] == \"B\" else 1\n",
    "        x = self.char2idx[row[random_start + self.length][2]]\n",
    "        y = self.char2idx[row[random_start + self.length][3]]\n",
    "        label = torch.tensor([dim * 361 + x * 19 + y], dtype=torch.float32)\n",
    "\n",
    "        return boards, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        Returns:\n",
    "            int: Number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get data at the given index.\n",
    "        Args:\n",
    "            idx (int): Index of the data sample.\n",
    "        Returns:\n",
    "            torch.Tensor: Processed and padded data sample.\n",
    "        \"\"\"\n",
    "        # Get data at the given index\n",
    "        row = self.data[idx]\n",
    "\n",
    "        # Transform data into a board\n",
    "        processed_data, label = self.__read_from_file(row)\n",
    "        return processed_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Conformer convolution module.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): input dimension.\n",
    "        num_channels (int): number of depthwise convolution layer input channels.\n",
    "        depthwise_kernel_size (int): kernel size of depthwise convolution layer.\n",
    "        dropout (float, optional): dropout probability. (Default: 0.0)\n",
    "        bias (bool, optional): indicates whether to add bias term to each convolution layer. (Default: ``False``)\n",
    "        use_group_norm (bool, optional): use GroupNorm rather than BatchNorm. (Default: ``False``)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_channels: int,\n",
    "        depthwise_kernel_size: int,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = False,\n",
    "        use_group_norm: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if (depthwise_kernel_size - 1) % 2 != 0:\n",
    "            raise ValueError(\n",
    "                \"depthwise_kernel_size must be odd to achieve 'SAME' padding.\"\n",
    "            )\n",
    "\n",
    "        # Layer normalization for input\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "        # Sequential layers: 1x1 Conv, GLU, Depthwise Conv, Normalization, Activation, 1x1 Conv, Dropout\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                input_dim,\n",
    "                2 * num_channels,\n",
    "                1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            nn.GLU(dim=1),\n",
    "            nn.Conv1d(\n",
    "                num_channels,\n",
    "                num_channels,\n",
    "                depthwise_kernel_size,\n",
    "                stride=1,\n",
    "                padding=(depthwise_kernel_size - 1) // 2,\n",
    "                groups=num_channels,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            nn.GroupNorm(num_groups=1, num_channels=num_channels)\n",
    "            if use_group_norm\n",
    "            else nn.BatchNorm1d(num_channels),\n",
    "            nn.SiLU(),  # SiLU activation function (Sigmoid Linear Unit)\n",
    "            nn.Conv1d(\n",
    "                num_channels,\n",
    "                input_dim,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Conformer convolution module.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor with shape `(B, T, D)`.\n",
    "            B: Batch size, T: Sequence length, D: Input dimension\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with shape `(B, T, D)`.\n",
    "        \"\"\"\n",
    "        x = self.layer_norm(input)\n",
    "        # Transpose to shape `(B, D, T)` for 1D convolutions\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.sequential(x)  # Apply sequential layers\n",
    "        return x.transpose(1, 2)  # Transpose back to shape `(B, T, D)`\n",
    "\n",
    "\n",
    "class FeedForwardModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Feedforward module with Layer Normalization, Linear layers, SiLU activation, and Dropout.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Input dimension.\n",
    "        hidden_dim (int): Hidden layer dimension.\n",
    "        dropout (float, optional): Dropout probability. (Default: 0.1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n",
    "        super(FeedForwardModule, self).__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.LayerNorm(input_dim),\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.SiLU(),  # SiLU activation function (Sigmoid Linear Unit)\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the FeedForwardModule.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape `(B, T, D)`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with the same shape as the input tensor.\n",
    "        \"\"\"\n",
    "        return self.module(x)\n",
    "\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Conformer layer that constitutes Conformer.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): input dimension.\n",
    "        ffn_dim (int): hidden layer dimension of the feedforward network.\n",
    "        num_attention_heads (int): number of attention heads.\n",
    "        depthwise_conv_kernel_size (int): kernel size of the depthwise convolution layer.\n",
    "        dropout (float, optional): dropout probability. (Default: 0.1)\n",
    "        use_group_norm (bool, optional): use ``GroupNorm`` rather than ``BatchNorm1d``\n",
    "            in the convolution module. (Default: ``False``)\n",
    "        convolution_first (bool, optional): apply the convolution module ahead of\n",
    "            the attention module. (Default: ``False``)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        ffn_dim,\n",
    "        num_attention_heads,\n",
    "        depthwise_conv_kernel_size,\n",
    "        dropout=0.1,\n",
    "        use_group_norm=False,\n",
    "        convolution_first=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ffn1 = FeedForwardModule(input_dim, ffn_dim, dropout)\n",
    "        self.ffn2 = FeedForwardModule(input_dim, ffn_dim, dropout)\n",
    "        self.conv = ConvModule(\n",
    "            input_dim,\n",
    "            input_dim,\n",
    "            depthwise_conv_kernel_size,\n",
    "            dropout,\n",
    "            use_group_norm=use_group_norm,\n",
    "        )\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            input_dim, num_attention_heads, dropout=dropout\n",
    "        )\n",
    "        self.self_attn_dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "        self.convolution_first = convolution_first\n",
    "\n",
    "    def __apply_conv(self, x):\n",
    "        \"\"\"\n",
    "        Apply the convolution module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape `(T, B, D)`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying the convolution module.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        # Transpose to shape `(B, T, D)` for 1D convolutions\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.conv(x)\n",
    "        x = x.transpose(0, 1)  # Transpose back to shape `(T, B, D)`\n",
    "        x = x + residual\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the ConformerBlock.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape `(T, B, D)`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with the same shape as the input tensor.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.ffn1(x)  # First feedforward module\n",
    "        x = 0.5 * x + residual  # Residual connection and scaling\n",
    "\n",
    "        if self.convolution_first:\n",
    "            x = self.__apply_conv(x)  # Apply convolution module if specified\n",
    "\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)  # Layer normalization\n",
    "        x, _ = self.self_attn(x, x, x)  # Multihead self-attention\n",
    "        x = self.self_attn_dropout(x)\n",
    "        x = x + residual  # Residual connection\n",
    "\n",
    "        if not self.convolution_first:\n",
    "            x = self.__apply_conv(x)  # Apply convolution module if specified\n",
    "\n",
    "        residual = x\n",
    "        x = self.ffn2(x)  # Second feedforward module\n",
    "        x = 0.5 * x + residual  # Residual connection and scaling\n",
    "        x = self.layer_norm(x)  # Final layer normalization\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dim (int): input dimension.\n",
    "        num_heads (int): number of attention heads in each Conformer layer.\n",
    "        ffn_dim (int): hidden layer dimension of feedforward networks.\n",
    "        num_layers (int): number of Conformer layers to instantiate.\n",
    "        depthwise_conv_kernel_size (int): kernel size of each Conformer layer's depthwise convolution layer.\n",
    "        dropout (float, optional): dropout probability. (Default: 0.1)\n",
    "        use_group_norm (bool, optional): use ``GroupNorm`` rather than ``BatchNorm1d``\n",
    "            in the convolution module. (Default: ``False``)\n",
    "        convolution_first (bool, optional): apply the convolution module ahead of\n",
    "            the attention module. (Default: ``False``)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        num_heads,\n",
    "        ffn_dim,\n",
    "        num_layers,\n",
    "        depthwise_conv_kernel_size,\n",
    "        dropout=0.1,\n",
    "        use_group_norm=False,\n",
    "        convolution_first=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Instantiate Conformer blocks\n",
    "        self.conformer_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConformerBlock(\n",
    "                    input_dim,\n",
    "                    ffn_dim,\n",
    "                    num_heads,\n",
    "                    depthwise_conv_kernel_size,\n",
    "                    dropout,\n",
    "                    use_group_norm,\n",
    "                    convolution_first,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the Generator (Conformer model).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input with shape `(B, T, input_dim)`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output with shape `(B, T, input_dim)`.\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _, _, _ = x.shape\n",
    "        x = x.view(batch_size, seq_length, -1)  # Flatten input tensor\n",
    "\n",
    "        x = x.transpose(0, 1)  # Transpose to shape `(T, B, input_dim)`\n",
    "\n",
    "        # Pass input through Conformer blocks\n",
    "        for layer in self.conformer_blocks:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = x.transpose(0, 1)  # Transpose back to shape `(B, T, input_dim)`\n",
    "\n",
    "        return x\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator model using Conformer architecture.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Input dimension.\n",
    "        num_heads (int): Number of attention heads in each Conformer layer.\n",
    "        ffn_dim (int): Hidden layer dimension of feedforward networks in Conformer layers.\n",
    "        num_layers (int): Number of Conformer layers.\n",
    "        depthwise_conv_kernel_size (int): Kernel size of depthwise convolution in Conformer layers.\n",
    "        dropout (float, optional): Dropout probability. (Default: 0.1)\n",
    "        use_group_norm (bool, optional): Use GroupNorm instead of BatchNorm1d in Conformer layers. (Default: False)\n",
    "        convolution_first (bool, optional): Apply convolution module ahead of attention module. (Default: False)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        num_heads,\n",
    "        ffn_dim,\n",
    "        num_layers,\n",
    "        depthwise_conv_kernel_size,\n",
    "        dropout=0.1,\n",
    "        use_group_norm=False,\n",
    "        convolution_first=False,\n",
    "    ):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Instantiate the Conformer module\n",
    "        self.conformer = Conformer(\n",
    "            input_dim,\n",
    "            num_heads,\n",
    "            ffn_dim,\n",
    "            num_layers,\n",
    "            depthwise_conv_kernel_size,\n",
    "            dropout,\n",
    "            use_group_norm,\n",
    "            convolution_first,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the Generator (Conformer model).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape `(B, T, input_dim)`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with shape `(B, output_dim)`.\n",
    "        \"\"\"\n",
    "        # Pass the input through the Conformer layers\n",
    "        conformer_output = self.conformer(x)\n",
    "\n",
    "        # truncate the output to the last time step\n",
    "        output = conformer_output[:, -1, :]\n",
    "\n",
    "        # Pass the output through the linear layer\n",
    "        output = self.output_layer(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goDataset = GoDataset(\"data/train/dan_train.csv\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "G_path='data/models/gen/2_epoch10.pth'\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"input_dim\": 2 * 19 * 19,\n",
    "    \"num_heads\": 2,\n",
    "    \"ffn_dim\": 512,\n",
    "    \"num_layers\": 4,\n",
    "    \"depthwise_conv_kernel_size\": 7,\n",
    "    \"dropout\": 0.1,\n",
    "    \"use_group_norm\": False,\n",
    "    \"convolution_first\": False,\n",
    "    \"lr\": 0.0001,\n",
    "    \"gen_path\": \"data/models/gen\",\n",
    "    \"dis_path\": \"data/models/dis\",\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"batch_size\": 32,\n",
    "    \"clip_value\": 1,\n",
    "    \"data_len\": 32,\n",
    "    \"epochs\": 200,\n",
    "    \"early_stop\": 200,\n",
    "    \"selected\": 0\n",
    "}\n",
    "\n",
    "goDataset = GoDataset(\"data/train/dan_train.csv\", config[\"data_len\"])\n",
    "val_loader = DataLoader(\n",
    "    goDataset, batch_size=int(config[\"batch_size\"]), shuffle=False, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def plot_boards(pos, y):\n",
    "    for i in range(len(pos)):\n",
    "        output = torch.zeros(19*19*2)\n",
    "        output[int(pos[i].item())] = 1\n",
    "        output = output.reshape(19, 19, 2)\n",
    "        plt.subplot(221)\n",
    "        plt.imshow(output[:,:,0])\n",
    "        plt.subplot(222)\n",
    "        plt.imshow(output[:,:,1])\n",
    "        output = torch.zeros(19*19*2)\n",
    "        output[int(y[i].item())] = 1\n",
    "        output = output.reshape(19, 19, 2)\n",
    "        plt.subplot(223)\n",
    "        plt.imshow(output[:,:,0])\n",
    "        plt.subplot(224)\n",
    "        plt.imshow(output[:,:,1])\n",
    "        plt.show()\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "\n",
    "model = torch.load(G_path).to(\"cpu\")\n",
    "\n",
    "def evaluate_G(model, val_loader):\n",
    "    print(f\"Evaluating generator:\")\n",
    "\n",
    "    # Set the generator and discriminator in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate through the validation loader\n",
    "    for i, (x, y) in enumerate(tqdm(val_loader)):\n",
    "        with torch.no_grad():\n",
    "            # Generate fake data and conditioning information from the generator\n",
    "            output = model(x)\n",
    "\n",
    "        plot_boards(output, y)\n",
    "\n",
    "\n",
    "evaluate_G(model, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('.venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fdb6ab0ea572449d4e6cd1b6c5ec84b94b32c6763ff106b98270581a4c7701e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
